{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name:** WANG Enrui\n",
    "\n",
    "**EID:** enruiwang2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS4487 - Tutorial 5: Predicting Popularity of Online News\n",
    "\n",
    "In this tutorial you will train regression models to predict the number of \"shares\" of a news article on Mashable.\n",
    "\n",
    "First we need to initialize Python.  Run the below cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import IPython.core.display         \n",
    "# setup output image format (Chrome works best)\n",
    "IPython.core.display.set_matplotlib_formats(\"svg\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from numpy import *\n",
    "from sklearn import *\n",
    "import glob\n",
    "import os\n",
    "import csv\n",
    "import string\n",
    "random.seed(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading Data and Pre-processing\n",
    "Next we need to load the data.  Download `OnlineNewsPopularity.zip`, and **unzip** it in the same directory as this ipynb file.  Then run the following cell to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'OnlineNewsPopularity/OnlineNewsPopularity.csv'\n",
    "\n",
    "# read the data\n",
    "allfeatnames = []\n",
    "textdata      = []\n",
    "with open(filename, 'r') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if len(allfeatnames)==0:\n",
    "            allfeatnames = row\n",
    "        else:\n",
    "            textdata.append(row)\n",
    "\n",
    "# put the data into a np array\n",
    "dataX = empty((len(textdata), len(allfeatnames)-3))\n",
    "dataY = empty(len(textdata))\n",
    "for i,row in enumerate(textdata):\n",
    "    # extract features (remove the first 2 features and the last feature)\n",
    "    dataX[i,:] = array([float(x) for x in row[2:-1]])\n",
    "    # extract target (last entry)\n",
    "    dataY[i] = float(row[-1])\n",
    "    \n",
    "# extract feature names\n",
    "featnames = [x.strip() for x in allfeatnames[2:-1]]\n",
    "\n",
    "# extract a subset of data\n",
    "dataX = dataX[::6]\n",
    "dataY = dataY[::6]\n",
    "\n",
    "print(dataX.shape)\n",
    "print(dataY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 58 features for each article.  Here are the feature names, and an example entry.  The actual description of the features can be found in the `OnlineNewsPopularity-features.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(featnames)\n",
    "\n",
    "print(\"--- example article features---\")\n",
    "print(dataX[0])\n",
    "print(\"--- example article target (# of shares)\")\n",
    "print(dataY[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now separate the data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly split data into 50% train and 50% test set\n",
    "trainX, testX, trainYo, testYo = \\\n",
    "  model_selection.train_test_split(dataX, dataY, \n",
    "  train_size=0.50, test_size=0.50, random_state=4487)\n",
    "\n",
    "print(trainX.shape)\n",
    "print(testX.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we normalize the input features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize feature values\n",
    "# this makes comparing weights more meaningful\n",
    "scaler = preprocessing.StandardScaler()  \n",
    "trainXn = scaler.fit_transform(trainX)  \n",
    "testXn  = scaler.transform(testX)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the target value (number of shares) has a large dynamic range, we will transform the target values through the log function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map targets to log-space\n",
    "trainY = log10(trainYo)\n",
    "testY  = log10(testYo)\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(1,2,1)\n",
    "plt.hist(trainYo, 25);\n",
    "plt.title('histogram of Y values')\n",
    "plt.subplot(1,2,2)\n",
    "plt.hist(trainY, 25);\n",
    "plt.title(\"histogram of log(Y) values\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prediction with Linear Regression\n",
    "\n",
    "First we will look at predicting the number of shares using simple linear regression models.  Use the training data to fit a linear model using Ordinary Least Squares and Ridge Regression.  Use cross-validation on the training set to select the optimal $\\alpha$ parameter for ridge regression, and plot the corresponding prediction accurancy on both training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE\n",
    "## HINT: \n",
    "# 1. Ordinary Least Squares: linear_model.LinearRegression()\n",
    "# 2. Ridge Regression: linear_model.Ridge(alphas= )\n",
    "# 3. Rigge Regression with Cross-validation: linear_model.Ridge(alphas= )\n",
    "# 4. plt.semilogx(alphas, trainAEs/testAEs)\n",
    "\n",
    "ols = linear_model.LinearRegression()\n",
    "ols.fit(trainXn,trainY)\n",
    "\n",
    "\n",
    "alphas =logspace(-3,4,50)\n",
    "rr = model_selection.GridSearchCV(linear_model.Ridge(),param_grid={'alpha':alphas}, cv=10, n_jobs=-1)\n",
    "rr.fit(trainXn,trainY)\n",
    "alpha=rr.best_params_\n",
    "print(alpha)\n",
    "\n",
    "trainAEs=[]\n",
    "testAEs=[]\n",
    "for alpha in alphas:\n",
    "    temp_rr=linear_model.Ridge(alpha=alpha)\n",
    "    temp_rr.fit(trainXn,trainY)\n",
    "    trainAEs.append(metrics.mean_absolute_error(trainY, temp_rr.predict(trainXn)))\n",
    "    testAEs.append(metrics.mean_absolute_error(testY, temp_rr.predict(testXn)))\n",
    "plt.title(\"Train\")\n",
    "plt.semilogx(alphas, trainAEs)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Test\")\n",
    "plt.semilogx(alphas, testAEs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the two models using the _average absolute error_ (AE) between the predictions and the true values.  Below is  code that will calculate AE for the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ols is the linear regression model\n",
    "trainAE = metrics.mean_absolute_error(trainY, ols.predict(trainXn))\n",
    "testAE  = metrics.mean_absolute_error(testY, ols.predict(testXn))\n",
    "print(\"OLS: train error =\", trainAE)\n",
    "print(\"OLS: test error =\", testAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rr is the ridge regression model\n",
    "trainAE = metrics.mean_absolute_error(trainY, rr.predict(trainXn))\n",
    "testAE  = metrics.mean_absolute_error(testY, rr.predict(testXn))\n",
    "print(\"RR: train error =\", trainAE)\n",
    "print(\"RR: test error =\", testAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model has better prediction ability on the test set? Why?\n",
    "- **INSERT YOUR ANSWER HERE**\n",
    "\n",
    "ridge regression model. After normalization, ols may not detect the relation of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Which features are important?\n",
    "Next we will investigate which features are the most important for the prediction.  Use LASSO with cross-validation to learn the model, and print the training and testing error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE\n",
    "## HINT\n",
    "# 1. LASSO with Cross-validation: linear_model.LassoCV()\n",
    "las= linear_model.LassoCV(cv=5, random_state=0)\n",
    "las.fit(trainXn,trainY)\n",
    "\n",
    "trainAE = metrics.mean_absolute_error(trainY, las.predict(trainXn))\n",
    "testAE  = metrics.mean_absolute_error(testY, las.predict(testXn))\n",
    "print(\"LASSO: train error =\", trainAE)\n",
    "print(\"LASSO: test error =\", testAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the LASSO coefficients by sorting them in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort coefficients from smallest to largest, then reverse it\n",
    "inds = argsort(abs(las.coef_))[::-1]\n",
    "# print out\n",
    "print(\"weight : feature description\")\n",
    "for i in inds:\n",
    "    print(\"{: .3f} : {}\".format(las.coef_[i], featnames[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Which features are most important for predicting the number of shares?  For these features, which feature values (low or high values) will yield a higher number of shares?_\n",
    "- **INSERT YOUR ANSWER HERE**\n",
    "\n",
    "top 5 important features are \"kw_avg_avg\",\"LDA_02\",\"LDA_02\",\"data_channel_is_entertainment\",\"average_token_length\".\n",
    "Higher value for \"kw_avg_avg\" will yield a higher number of shares.\n",
    "Lower value for \"LDA_02\",\"LDA_02\",\"data_channel_is_entertainment\",\"average_token_length\" will yield a higher number of shares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Kernel Methods and Supprot Vector Regression \n",
    "Next, let us try some non-linear regression model such as[ kernel ridge regression](https://scikit-learn.org/stable/modules/kernel_ridge.html), [random forest regression](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) (Optional), [support vector regression](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html). To improve the prediction accuracy in Tutorial 4, and using cross-validation on the training set to select the parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT YOUR CODE HERE\n",
    "##Hint: \n",
    "## 1. Kernel Ridge Regression:  kernel_ridge.KernelRidge(kernel='rbf'/'poly')\n",
    "## 2. Random Forest Regression: ensemble.RandomForestRegressor(random_state= , n_estimators= )\n",
    "## 3. Support Vector Regression: svm.SVR(kernel='poly')\n",
    "## 4. Avoid using large values of $C$ with SVR.\n",
    "exps = {\n",
    "    'kr-rbf': {\n",
    "        'paramgrid': {'alpha': logspace(-2,3,10),'gamma': logspace(-4,3,10)},\n",
    "        'clf': kernel_ridge.KernelRidge(kernel='rbf') },\n",
    "    'kr-poly': {\n",
    "        'paramgrid': {'alpha': logspace(-2,3,10),'gamma': logspace(-4,3,10)},\n",
    "        'clf': kernel_ridge.KernelRidge(kernel='poly') },\n",
    "    'rfr': {\n",
    "        'paramgrid': {'n_estimators': arange(1, 20, 1), 'max_depth': arange(1, 20, 1) },\n",
    "        'clf': ensemble.RandomForestRegressor() },\n",
    "    'svr-poly': {\n",
    "        'paramgrid': {'C': logspace(-2,3,10), 'degree': [2, 3, 4] },\n",
    "        'clf': svm.SVR(kernel='poly') },\n",
    "    }\n",
    "clfs= {}\n",
    "trainAE={}\n",
    "testAE={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myname = 'kr-rbf'\n",
    "clfs[myname] = model_selection.GridSearchCV(exps[myname]['clf'],param_grid=exps[myname]['paramgrid'], cv=5, n_jobs=-1)\n",
    "clfs[myname].fit(trainXn,trainY)\n",
    "print(\"best parameters for KernelRidge-RBF are:\",clfs[myname].best_params_)\n",
    "trainAE[myname] = metrics.mean_absolute_error(trainY, clfs[myname].predict(trainXn))\n",
    "testAE[myname]  = metrics.mean_absolute_error(testY, clfs[myname].predict(testXn))\n",
    "print(\"train error:\", trainAE[myname])\n",
    "print(\"test error :\", testAE[myname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myname = 'kr-poly'\n",
    "clfs[myname] = model_selection.GridSearchCV(exps[myname]['clf'],param_grid=exps[myname]['paramgrid'], cv=5, n_jobs=-1)\n",
    "clfs[myname].fit(trainXn,trainY)\n",
    "print(\"best parameters for KernelRidge-Poly are:\",clfs[myname].best_params_)\n",
    "trainAE[myname] = metrics.mean_absolute_error(trainY, clfs[myname].predict(trainXn))\n",
    "testAE[myname]  = metrics.mean_absolute_error(testY, clfs[myname].predict(testXn))\n",
    "print(\"train error:\", trainAE[myname])\n",
    "print(\"test error :\", testAE[myname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myname = 'rfr'\n",
    "clfs[myname] = model_selection.GridSearchCV(exps[myname]['clf'],param_grid=exps[myname]['paramgrid'], cv=5, n_jobs=-1)\n",
    "clfs[myname].fit(trainXn,trainY)\n",
    "print(\"best parameters for RandomForestRegressor are:\",clfs[myname].best_params_)\n",
    "trainAE[myname] = metrics.mean_absolute_error(trainY, clfs[myname].predict(trainXn))\n",
    "testAE[myname]  = metrics.mean_absolute_error(testY, clfs[myname].predict(testXn))\n",
    "print(\"train error:\", trainAE[myname])\n",
    "print(\"test error :\", testAE[myname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myname = 'svr-poly'\n",
    "clfs[myname] = model_selection.GridSearchCV(exps[myname]['clf'],param_grid=exps[myname]['paramgrid'], cv=5, n_jobs=-1)\n",
    "clfs[myname].fit(trainXn,trainY)\n",
    "print(\"best parameters for SVR are:\",clfs[myname].best_params_)\n",
    "trainAE[myname] = metrics.mean_absolute_error(trainY, clfs[myname].predict(trainXn))\n",
    "testAE[myname]  = metrics.mean_absolute_error(testY, clfs[myname].predict(testXn))\n",
    "print(\"train error:\", trainAE[myname])\n",
    "print(\"test error :\", testAE[myname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LASSO - poly features\n",
    "polyfeats = preprocessing.PolynomialFeatures(degree=2)\n",
    "trainXnf  = polyfeats.fit_transform(trainXn)\n",
    "testXnf   = polyfeats.transform(testXn) \n",
    "\n",
    "myname = 'las-poly'\n",
    "clfs[myname] = linear_model.LassoCV(max_iter=2000,tol=1)\n",
    "clfs[myname].fit(trainXnf, trainY)\n",
    "trainAE[myname] = metrics.mean_absolute_error(trainY, clfs[myname].predict(trainXnf))\n",
    "testAE[myname]  = metrics.mean_absolute_error(testY, clfs[myname].predict(testXnf))\n",
    "\n",
    "print(\"train error:\", trainAE[myname])\n",
    "print(\"test error :\", testAE[myname])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort coefficients from smallest to largest, then reverse it\n",
    "inds = argsort(abs(clfs['las-poly'].coef_))[::-1]\n",
    "print(len(polyfeats.powers_))\n",
    "# print out\n",
    "print(\"weight : feature description\")\n",
    "for i in inds:\n",
    "    if abs(clfs['las-poly'].coef_[i])>1e-3:\n",
    "        # get active features and powers\n",
    "        pows = where(polyfeats.powers_[i])[0]\n",
    "        fstr = \"\"\n",
    "        for p in pows:\n",
    "            fstr += featnames[p] + \"(\" + str(polyfeats.powers_[i][p]) + \") \"\n",
    "        print(\"{: .3f} : {}\".format(clfs['las-poly'].coef_[i], fstr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Which regression method performs the best? Why do you think so?_\n",
    "- **INSERT YOUR ANSWER HERE**\n",
    "KernelRidge-Poly. The error is the smallest"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
